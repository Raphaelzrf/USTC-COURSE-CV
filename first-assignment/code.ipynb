{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241c003f",
   "metadata": {},
   "source": [
    "# import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d21c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import tracemalloc\n",
    "from enum import Enum\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import copy\n",
    "import json\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d2596",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8e2e4",
   "metadata": {},
   "source": [
    "## Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fd87f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the enums for hyperparameters\n",
    "class NumOfHiddenLayer(Enum):\n",
    "    THREE = 3\n",
    "    FOUR = 4\n",
    "    FIVE = 5\n",
    "\n",
    "class LearningRateScheduler(Enum):\n",
    "    FIXED = 'None'\n",
    "    StepLR = 'StepLR'\n",
    "    CosineAnnealingLR = 'CosineAnnealingLR'\n",
    "    ReduceLROnPlateau = 'ReduceLROnPlateau'\n",
    "\n",
    "class ActivationFunction(Enum):\n",
    "    ReLU = \"ReLU\"\n",
    "    LeakyReLU = \"LeakyReLU\"\n",
    "    ELU = \"ELU\"\n",
    "class Optimizer(Enum):\n",
    "    SGD = 'SGD'\n",
    "    Adam = 'Adam'\n",
    "    RMSprop = 'RMSprop'\n",
    "class BatchNormalization(Enum):\n",
    "    True_ = True\n",
    "    False_ = False\n",
    "class Regularization(Enum):\n",
    "    L1 = 'L1'\n",
    "    L2 = 'L2'\n",
    "    None_ = None\n",
    "class Augmentation(Enum):\n",
    "    True_ = True\n",
    "    False_ = False\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),         # rotate \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # translate\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x)),  # add gaussian noise\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])  \n",
    "# define the hyperparameters\n",
    "num_of_hidden_layer = NumOfHiddenLayer.THREE\n",
    "# hidden_sizes = [32, 32, 32]\n",
    "# hidden_sizes = [64, 64, 64]\n",
    "hidden_sizes = [128, 128, 128]\n",
    "learning_rate_scheduler = LearningRateScheduler.StepLR\n",
    "activation_function = ActivationFunction.ELU\n",
    "optimizer = Optimizer.SGD\n",
    "batch_normalization = BatchNormalization.True_\n",
    "regularization = Regularization.L1\n",
    "llambda = 1e-5\n",
    "dropout = [0.25, 0.25, 0.0]\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "lr = 0.1\n",
    "augmentation = Augmentation.True_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa902ce",
   "metadata": {},
   "source": [
    "## download EMNIST dataset and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a04eff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset EMNIST\n",
      "    Number of datapoints: 112800\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n",
      "Dataset EMNIST\n",
      "    Number of datapoints: 18800\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # EMNIST normalized\n",
    "])\n",
    "\n",
    "train_dataset = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473253d8",
   "metadata": {},
   "source": [
    "## plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5308cf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(dataset, num_samples=16, cols=4):\n",
    "    \"\"\"plot a portion of the sample in the dataset\"\"\"\n",
    "    # samples are selected at random\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    images = [dataset[i][0].squeeze().numpy() for i in indices]  # remove the channel dimension\n",
    "    labels = [dataset.classes[dataset[i][1]] for i in indices]   # get the tag name\n",
    "\n",
    "    # set up the canvas\n",
    "    rows = num_samples // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 2 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # draw an image\n",
    "    for img, label, ax in zip(images, labels, axes):\n",
    "        ax.imshow(img.T, cmap='gray')  # note emnist images need to be transposed\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a920d2",
   "metadata": {},
   "source": [
    "## Define the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7be27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: int = 28*28, \n",
    "        hidden_sizes: List[int] = [128, 128, 128], \n",
    "        num_classes: int = 47, \n",
    "        activation_fn: nn.Module = nn.ReLU, \n",
    "        batch_normalization: bool = True, \n",
    "        dropout: List[float] = None, \n",
    "    ) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        all_layes_sizes = [input_size] + hidden_sizes + [num_classes]\n",
    "        for i in range(len(all_layes_sizes)-2):\n",
    "            self.net.add_module(\n",
    "                f\"fc{i}\", \n",
    "                nn.Linear(all_layes_sizes[i], all_layes_sizes[i+1])\n",
    "            )\n",
    "            if batch_normalization:\n",
    "                self.net.add_module(f\"bn{i}\", nn.BatchNorm1d(all_layes_sizes[i+1]))\n",
    "            self.net.add_module(f\"activation{i}\", activation_fn())\n",
    "            if dropout is not None and dropout[i] > 1e-4:\n",
    "                self.net.add_module(f\"dropout{i}\", nn.Dropout(dropout[i]))\n",
    "        self.net.add_module(\"output\", nn.Linear(all_layes_sizes[-2], all_layes_sizes[-1])) # output layer\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1) \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb439b0",
   "metadata": {},
   "source": [
    "## define the function to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b2db6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, optimizer: Optimizer, scheduler: _LRScheduler, criterion: nn.Module, num_epochs: int, device: torch.device, model_path: str = 'best_model.pth') -> Tuple[List[float], List[float], List[float], List[float], List[float], List[float], List[float], List[float]]:\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0\n",
    "    best_train_loss = float('inf')\n",
    "    best_train_acc = 0\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    num_of_no_improvement = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            # add regularization\n",
    "            if regularization == Regularization.L1:\n",
    "                l1_lambda = 1e-5\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += l1_lambda * l1_norm\n",
    "            elif regularization == Regularization.L2:\n",
    "                l2_lambda = 1e-4\n",
    "                l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "                loss += l2_lambda * l2_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct / total\n",
    "        val_acc_list.append(val_acc)\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "        if learning_rate_scheduler == LearningRateScheduler.StepLR:\n",
    "            scheduler.step()\n",
    "        elif learning_rate_scheduler == LearningRateScheduler.ReduceLROnPlateau:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_train_loss = train_loss\n",
    "            best_train_acc = train_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            num_of_no_improvement = 0\n",
    "        else:\n",
    "            num_of_no_improvement += 1\n",
    "            if num_of_no_improvement >= 10:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        epoch_end = time.time()\n",
    "        epoch_duration = epoch_end - epoch_start\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            peak_mem = torch.cuda.max_memory_allocated(device) / 1024 / 1024  # MB\n",
    "            torch.cuda.reset_peak_memory_stats(device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Time: {epoch_duration:.2f}s, \"\n",
    "              f\"Peak Memory: {peak_mem:.2f}MB\" if torch.cuda.is_available() else \"\")\n",
    "    return best_val_loss, best_val_acc, best_train_loss, best_train_acc, train_loss_list, val_loss_list, train_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd74c1",
   "metadata": {},
   "source": [
    "## start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup:\n",
      "num_of_hidden_layer: 3, \n",
      "hidden_sizes: [128, 128, 128], \n",
      "learning_rate: 0.1, \n",
      "learning_rate_scheduler: StepLR, \n",
      "optimizer: SGD, \n",
      "activation_function: ELU, \n",
      "batch_normalization: True, \n",
      "regularization: L1, \n",
      "llamada: 1e-05, \n",
      "dropout: [0.25, 0.25, 0.0], \n",
      "augmentation: True, \n",
      "num_of_epochs: 200, \n",
      "batch_size: 128, \n",
      "num_of_folds: 5, \n",
      "augmentation: True, \n",
      "train_dataset_size: 112800, \n",
      "test_dataset_size: 18800\n",
      "\n",
      "Fold 1\n",
      "Model already exists at ./models/best_model_fold_1-5_num-of-hidden-layer_3_hidden-sizes_[128, 128, 128]_lr_0.1_scheduler_StepLR_optimizer_SGD_activation_function_ELU_batch_normalization_True_regularization_L1_llambda_1e-05_dropout_[0.25, 0.25, 0.0]_aug.pth, skipping training.\n",
      "Epoch 1/200, Train Loss: 2.0148, Train Acc: 0.4345, Val Loss: 0.9182, Val Acc: 0.7142, Time: 57.32s, Peak Memory: 26.98MB\n",
      "Epoch 2/200, Train Loss: 1.5101, Train Acc: 0.5664, Val Loss: 0.7449, Val Acc: 0.7671, Time: 56.98s, Peak Memory: 26.98MB\n",
      "Epoch 3/200, Train Loss: 1.3724, Train Acc: 0.6062, Val Loss: 0.6987, Val Acc: 0.7699, Time: 57.04s, Peak Memory: 26.98MB\n",
      "Epoch 4/200, Train Loss: 1.2998, Train Acc: 0.6287, Val Loss: 0.6309, Val Acc: 0.7947, Time: 56.89s, Peak Memory: 26.98MB\n",
      "Epoch 5/200, Train Loss: 1.2535, Train Acc: 0.6437, Val Loss: 0.6085, Val Acc: 0.7991, Time: 56.99s, Peak Memory: 26.98MB\n",
      "Epoch 6/200, Train Loss: 1.2197, Train Acc: 0.6524, Val Loss: 0.5882, Val Acc: 0.8044, Time: 57.01s, Peak Memory: 26.98MB\n",
      "Epoch 7/200, Train Loss: 1.1939, Train Acc: 0.6610, Val Loss: 0.5787, Val Acc: 0.7972, Time: 56.21s, Peak Memory: 26.98MB\n",
      "Epoch 8/200, Train Loss: 1.1697, Train Acc: 0.6697, Val Loss: 0.5586, Val Acc: 0.8104, Time: 56.99s, Peak Memory: 26.98MB\n",
      "Epoch 9/200, Train Loss: 1.1512, Train Acc: 0.6753, Val Loss: 0.5384, Val Acc: 0.8172, Time: 57.04s, Peak Memory: 26.98MB\n",
      "Epoch 10/200, Train Loss: 1.1444, Train Acc: 0.6782, Val Loss: 0.5499, Val Acc: 0.8145, Time: 56.87s, Peak Memory: 26.98MB\n",
      "Epoch 11/200, Train Loss: 1.0684, Train Acc: 0.7008, Val Loss: 0.4879, Val Acc: 0.8350, Time: 56.83s, Peak Memory: 26.98MB\n",
      "Epoch 12/200, Train Loss: 1.0463, Train Acc: 0.7051, Val Loss: 0.4832, Val Acc: 0.8372, Time: 56.89s, Peak Memory: 26.98MB\n",
      "Epoch 13/200, Train Loss: 1.0401, Train Acc: 0.7076, Val Loss: 0.4823, Val Acc: 0.8363, Time: 56.18s, Peak Memory: 26.98MB\n",
      "Epoch 14/200, Train Loss: 1.0336, Train Acc: 0.7093, Val Loss: 0.4760, Val Acc: 0.8390, Time: 57.00s, Peak Memory: 26.98MB\n",
      "Epoch 15/200, Train Loss: 1.0285, Train Acc: 0.7092, Val Loss: 0.4866, Val Acc: 0.8300, Time: 56.95s, Peak Memory: 26.98MB\n",
      "Epoch 16/200, Train Loss: 1.0184, Train Acc: 0.7133, Val Loss: 0.4671, Val Acc: 0.8407, Time: 56.94s, Peak Memory: 26.98MB\n",
      "Epoch 17/200, Train Loss: 1.0145, Train Acc: 0.7138, Val Loss: 0.4802, Val Acc: 0.8346, Time: 56.38s, Peak Memory: 26.98MB\n",
      "Epoch 18/200, Train Loss: 1.0115, Train Acc: 0.7165, Val Loss: 0.4646, Val Acc: 0.8409, Time: 56.87s, Peak Memory: 26.98MB\n",
      "Epoch 19/200, Train Loss: 1.0088, Train Acc: 0.7159, Val Loss: 0.4631, Val Acc: 0.8422, Time: 57.00s, Peak Memory: 26.98MB\n",
      "Epoch 20/200, Train Loss: 1.0055, Train Acc: 0.7163, Val Loss: 0.4688, Val Acc: 0.8392, Time: 57.08s, Peak Memory: 26.98MB\n",
      "Epoch 21/200, Train Loss: 0.9755, Train Acc: 0.7257, Val Loss: 0.4459, Val Acc: 0.8478, Time: 57.02s, Peak Memory: 26.98MB\n",
      "Epoch 22/200, Train Loss: 0.9670, Train Acc: 0.7293, Val Loss: 0.4441, Val Acc: 0.8496, Time: 57.10s, Peak Memory: 26.98MB\n",
      "Epoch 23/200, Train Loss: 0.9566, Train Acc: 0.7314, Val Loss: 0.4452, Val Acc: 0.8445, Time: 56.81s, Peak Memory: 26.98MB\n",
      "Epoch 24/200, Train Loss: 0.9589, Train Acc: 0.7304, Val Loss: 0.4402, Val Acc: 0.8492, Time: 56.16s, Peak Memory: 26.98MB\n",
      "Epoch 25/200, Train Loss: 0.9451, Train Acc: 0.7334, Val Loss: 0.4375, Val Acc: 0.8515, Time: 57.06s, Peak Memory: 26.98MB\n",
      "Epoch 26/200, Train Loss: 0.9439, Train Acc: 0.7336, Val Loss: 0.4346, Val Acc: 0.8500, Time: 56.26s, Peak Memory: 26.98MB\n",
      "Epoch 27/200, Train Loss: 0.9449, Train Acc: 0.7353, Val Loss: 0.4336, Val Acc: 0.8539, Time: 57.04s, Peak Memory: 26.98MB\n",
      "Epoch 28/200, Train Loss: 0.9444, Train Acc: 0.7338, Val Loss: 0.4347, Val Acc: 0.8527, Time: 57.05s, Peak Memory: 26.98MB\n",
      "Epoch 29/200, Train Loss: 0.9393, Train Acc: 0.7328, Val Loss: 0.4347, Val Acc: 0.8522, Time: 57.01s, Peak Memory: 26.98MB\n",
      "Epoch 30/200, Train Loss: 0.9406, Train Acc: 0.7341, Val Loss: 0.4378, Val Acc: 0.8505, Time: 56.90s, Peak Memory: 26.98MB\n",
      "Epoch 31/200, Train Loss: 0.9260, Train Acc: 0.7389, Val Loss: 0.4265, Val Acc: 0.8542, Time: 57.01s, Peak Memory: 26.98MB\n",
      "Epoch 32/200, Train Loss: 0.9191, Train Acc: 0.7418, Val Loss: 0.4255, Val Acc: 0.8536, Time: 57.02s, Peak Memory: 26.98MB\n",
      "Epoch 33/200, Train Loss: 0.9143, Train Acc: 0.7436, Val Loss: 0.4209, Val Acc: 0.8572, Time: 57.03s, Peak Memory: 26.98MB\n",
      "Epoch 34/200, Train Loss: 0.9134, Train Acc: 0.7439, Val Loss: 0.4228, Val Acc: 0.8558, Time: 57.09s, Peak Memory: 26.98MB\n",
      "Epoch 35/200, Train Loss: 0.9134, Train Acc: 0.7439, Val Loss: 0.4207, Val Acc: 0.8561, Time: 57.04s, Peak Memory: 26.98MB\n",
      "Epoch 36/200, Train Loss: 0.9113, Train Acc: 0.7428, Val Loss: 0.4259, Val Acc: 0.8549, Time: 56.72s, Peak Memory: 26.98MB\n",
      "Epoch 37/200, Train Loss: 0.9081, Train Acc: 0.7431, Val Loss: 0.4243, Val Acc: 0.8535, Time: 56.77s, Peak Memory: 26.98MB\n",
      "Epoch 38/200, Train Loss: 0.9111, Train Acc: 0.7430, Val Loss: 0.4214, Val Acc: 0.8559, Time: 56.65s, Peak Memory: 26.98MB\n",
      "Epoch 39/200, Train Loss: 0.9045, Train Acc: 0.7454, Val Loss: 0.4229, Val Acc: 0.8541, Time: 56.62s, Peak Memory: 26.98MB\n",
      "Epoch 40/200, Train Loss: 0.9081, Train Acc: 0.7431, Val Loss: 0.4226, Val Acc: 0.8554, Time: 56.74s, Peak Memory: 26.98MB\n",
      "Epoch 41/200, Train Loss: 0.9002, Train Acc: 0.7460, Val Loss: 0.4152, Val Acc: 0.8600, Time: 56.66s, Peak Memory: 26.98MB\n",
      "Epoch 42/200, Train Loss: 0.8947, Train Acc: 0.7473, Val Loss: 0.4173, Val Acc: 0.8572, Time: 56.68s, Peak Memory: 26.98MB\n",
      "Epoch 43/200, Train Loss: 0.8938, Train Acc: 0.7484, Val Loss: 0.4131, Val Acc: 0.8585, Time: 56.64s, Peak Memory: 26.98MB\n",
      "Epoch 44/200, Train Loss: 0.8947, Train Acc: 0.7479, Val Loss: 0.4133, Val Acc: 0.8588, Time: 55.92s, Peak Memory: 26.98MB\n",
      "Epoch 45/200, Train Loss: 0.8851, Train Acc: 0.7493, Val Loss: 0.4110, Val Acc: 0.8601, Time: 55.74s, Peak Memory: 26.98MB\n",
      "Epoch 46/200, Train Loss: 0.8868, Train Acc: 0.7492, Val Loss: 0.4146, Val Acc: 0.8570, Time: 56.61s, Peak Memory: 26.98MB\n",
      "Epoch 47/200, Train Loss: 0.8893, Train Acc: 0.7474, Val Loss: 0.4141, Val Acc: 0.8575, Time: 56.62s, Peak Memory: 26.98MB\n",
      "Epoch 48/200, Train Loss: 0.8838, Train Acc: 0.7477, Val Loss: 0.4131, Val Acc: 0.8582, Time: 55.72s, Peak Memory: 26.98MB\n",
      "Epoch 49/200, Train Loss: 0.8858, Train Acc: 0.7495, Val Loss: 0.4122, Val Acc: 0.8595, Time: 55.69s, Peak Memory: 26.98MB\n",
      "Epoch 50/200, Train Loss: 0.8843, Train Acc: 0.7499, Val Loss: 0.4151, Val Acc: 0.8582, Time: 56.59s, Peak Memory: 26.98MB\n",
      "Epoch 51/200, Train Loss: 0.8824, Train Acc: 0.7505, Val Loss: 0.4112, Val Acc: 0.8596, Time: 56.65s, Peak Memory: 26.98MB\n",
      "Epoch 52/200, Train Loss: 0.8845, Train Acc: 0.7494, Val Loss: 0.4092, Val Acc: 0.8598, Time: 56.82s, Peak Memory: 26.98MB\n",
      "Epoch 53/200, Train Loss: 0.8800, Train Acc: 0.7499, Val Loss: 0.4103, Val Acc: 0.8590, Time: 56.66s, Peak Memory: 26.98MB\n",
      "Epoch 54/200, Train Loss: 0.8779, Train Acc: 0.7518, Val Loss: 0.4093, Val Acc: 0.8598, Time: 56.65s, Peak Memory: 26.98MB\n",
      "Epoch 55/200, Train Loss: 0.8789, Train Acc: 0.7498, Val Loss: 0.4091, Val Acc: 0.8608, Time: 56.70s, Peak Memory: 26.98MB\n",
      "Epoch 56/200, Train Loss: 0.8780, Train Acc: 0.7510, Val Loss: 0.4061, Val Acc: 0.8611, Time: 56.67s, Peak Memory: 26.98MB\n",
      "Epoch 57/200, Train Loss: 0.8789, Train Acc: 0.7516, Val Loss: 0.4085, Val Acc: 0.8598, Time: 56.58s, Peak Memory: 26.98MB\n",
      "Epoch 58/200, Train Loss: 0.8716, Train Acc: 0.7522, Val Loss: 0.4078, Val Acc: 0.8608, Time: 56.68s, Peak Memory: 26.98MB\n",
      "Epoch 59/200, Train Loss: 0.8719, Train Acc: 0.7515, Val Loss: 0.4064, Val Acc: 0.8609, Time: 56.61s, Peak Memory: 26.98MB\n",
      "Epoch 60/200, Train Loss: 0.8767, Train Acc: 0.7514, Val Loss: 0.4089, Val Acc: 0.8606, Time: 56.69s, Peak Memory: 26.98MB\n",
      "Epoch 61/200, Train Loss: 0.8758, Train Acc: 0.7520, Val Loss: 0.4121, Val Acc: 0.8588, Time: 56.62s, Peak Memory: 26.98MB\n",
      "Epoch 62/200, Train Loss: 0.8738, Train Acc: 0.7511, Val Loss: 0.4059, Val Acc: 0.8618, Time: 56.65s, Peak Memory: 26.98MB\n",
      "Epoch 63/200, Train Loss: 0.8736, Train Acc: 0.7523, Val Loss: 0.4065, Val Acc: 0.8613, Time: 56.63s, Peak Memory: 26.98MB\n",
      "Epoch 64/200, Train Loss: 0.8730, Train Acc: 0.7538, Val Loss: 0.4074, Val Acc: 0.8613, Time: 56.65s, Peak Memory: 26.98MB\n",
      "Epoch 65/200, Train Loss: 0.8723, Train Acc: 0.7540, Val Loss: 0.4060, Val Acc: 0.8607, Time: 56.64s, Peak Memory: 26.98MB\n",
      "Epoch 66/200, Train Loss: 0.8698, Train Acc: 0.7530, Val Loss: 0.4089, Val Acc: 0.8596, Time: 56.67s, Peak Memory: 26.98MB\n",
      "Epoch 67/200, Train Loss: 0.8710, Train Acc: 0.7529, Val Loss: 0.4060, Val Acc: 0.8611, Time: 56.65s, Peak Memory: 26.98MB\n",
      "Epoch 68/200, Train Loss: 0.8737, Train Acc: 0.7527, Val Loss: 0.4078, Val Acc: 0.8602, Time: 56.72s, Peak Memory: 26.98MB\n",
      "Epoch 69/200, Train Loss: 0.8739, Train Acc: 0.7533, Val Loss: 0.4054, Val Acc: 0.8617, Time: 56.71s, Peak Memory: 26.98MB\n",
      "Epoch 70/200, Train Loss: 0.8740, Train Acc: 0.7531, Val Loss: 0.4064, Val Acc: 0.8609, Time: 56.12s, Peak Memory: 26.98MB\n",
      "Epoch 71/200, Train Loss: 0.8731, Train Acc: 0.7524, Val Loss: 0.4056, Val Acc: 0.8614, Time: 56.62s, Peak Memory: 26.98MB\n",
      "Epoch 72/200, Train Loss: 0.8687, Train Acc: 0.7540, Val Loss: 0.4049, Val Acc: 0.8620, Time: 56.64s, Peak Memory: 26.98MB\n",
      "Epoch 73/200, Train Loss: 0.8720, Train Acc: 0.7539, Val Loss: 0.4065, Val Acc: 0.8609, Time: 56.66s, Peak Memory: 26.98MB\n",
      "Epoch 74/200, Train Loss: 0.8692, Train Acc: 0.7538, Val Loss: 0.4068, Val Acc: 0.8607, Time: 56.68s, Peak Memory: 26.98MB\n",
      "Epoch 75/200, Train Loss: 0.8725, Train Acc: 0.7531, Val Loss: 0.4066, Val Acc: 0.8603, Time: 56.69s, Peak Memory: 26.98MB\n",
      "Epoch 76/200, Train Loss: 0.8691, Train Acc: 0.7534, Val Loss: 0.4062, Val Acc: 0.8619, Time: 56.70s, Peak Memory: 26.98MB\n",
      "Epoch 77/200, Train Loss: 0.8706, Train Acc: 0.7546, Val Loss: 0.4055, Val Acc: 0.8613, Time: 56.85s, Peak Memory: 26.98MB\n",
      "Epoch 78/200, Train Loss: 0.8704, Train Acc: 0.7550, Val Loss: 0.4047, Val Acc: 0.8606, Time: 56.70s, Peak Memory: 26.98MB\n",
      "Epoch 79/200, Train Loss: 0.8710, Train Acc: 0.7529, Val Loss: 0.4081, Val Acc: 0.8598, Time: 56.84s, Peak Memory: 26.98MB\n",
      "Epoch 80/200, Train Loss: 0.8679, Train Acc: 0.7546, Val Loss: 0.4044, Val Acc: 0.8613, Time: 57.04s, Peak Memory: 26.98MB\n",
      "Epoch 81/200, Train Loss: 0.8718, Train Acc: 0.7527, Val Loss: 0.4050, Val Acc: 0.8613, Time: 57.01s, Peak Memory: 26.98MB\n",
      "Epoch 82/200, Train Loss: 0.8632, Train Acc: 0.7556, Val Loss: 0.4063, Val Acc: 0.8610, Time: 57.14s, Peak Memory: 26.98MB\n",
      "Epoch 83/200, Train Loss: 0.8648, Train Acc: 0.7556, Val Loss: 0.4054, Val Acc: 0.8613, Time: 57.13s, Peak Memory: 26.98MB\n",
      "Epoch 84/200, Train Loss: 0.8708, Train Acc: 0.7538, Val Loss: 0.4048, Val Acc: 0.8609, Time: 57.21s, Peak Memory: 26.98MB\n",
      "Epoch 85/200, Train Loss: 0.8714, Train Acc: 0.7527, Val Loss: 0.4053, Val Acc: 0.8616, Time: 57.12s, Peak Memory: 26.98MB\n",
      "Epoch 86/200, Train Loss: 0.8664, Train Acc: 0.7539, Val Loss: 0.4046, Val Acc: 0.8613, Time: 57.10s, Peak Memory: 26.98MB\n",
      "Epoch 87/200, Train Loss: 0.8667, Train Acc: 0.7540, Val Loss: 0.4048, Val Acc: 0.8613, Time: 57.14s, Peak Memory: 26.98MB\n",
      "Epoch 88/200, Train Loss: 0.8685, Train Acc: 0.7533, Val Loss: 0.4035, Val Acc: 0.8621, Time: 56.57s, Peak Memory: 26.98MB\n",
      "Epoch 89/200, Train Loss: 0.8686, Train Acc: 0.7549, Val Loss: 0.4053, Val Acc: 0.8610, Time: 57.10s, Peak Memory: 26.98MB\n",
      "Epoch 90/200, Train Loss: 0.8696, Train Acc: 0.7547, Val Loss: 0.4041, Val Acc: 0.8615, Time: 57.10s, Peak Memory: 26.98MB\n",
      "Epoch 91/200, Train Loss: 0.8632, Train Acc: 0.7568, Val Loss: 0.4054, Val Acc: 0.8611, Time: 57.14s, Peak Memory: 26.98MB\n",
      "Epoch 92/200, Train Loss: 0.8626, Train Acc: 0.7540, Val Loss: 0.4050, Val Acc: 0.8618, Time: 57.12s, Peak Memory: 26.98MB\n",
      "Epoch 93/200, Train Loss: 0.8600, Train Acc: 0.7559, Val Loss: 0.4032, Val Acc: 0.8624, Time: 57.13s, Peak Memory: 26.98MB\n",
      "Epoch 94/200, Train Loss: 0.8709, Train Acc: 0.7531, Val Loss: 0.4051, Val Acc: 0.8617, Time: 57.20s, Peak Memory: 26.98MB\n",
      "Epoch 95/200, Train Loss: 0.8654, Train Acc: 0.7535, Val Loss: 0.4052, Val Acc: 0.8611, Time: 57.12s, Peak Memory: 26.98MB\n",
      "Epoch 96/200, Train Loss: 0.8664, Train Acc: 0.7537, Val Loss: 0.4046, Val Acc: 0.8621, Time: 57.22s, Peak Memory: 26.98MB\n",
      "Epoch 97/200, Train Loss: 0.8657, Train Acc: 0.7551, Val Loss: 0.4053, Val Acc: 0.8613, Time: 57.09s, Peak Memory: 26.98MB\n",
      "Epoch 98/200, Train Loss: 0.8688, Train Acc: 0.7541, Val Loss: 0.4066, Val Acc: 0.8612, Time: 57.08s, Peak Memory: 26.98MB\n",
      "Epoch 99/200, Train Loss: 0.8705, Train Acc: 0.7543, Val Loss: 0.4037, Val Acc: 0.8624, Time: 57.06s, Peak Memory: 26.98MB\n",
      "Epoch 100/200, Train Loss: 0.8666, Train Acc: 0.7544, Val Loss: 0.4041, Val Acc: 0.8620, Time: 57.07s, Peak Memory: 26.98MB\n",
      "Epoch 101/200, Train Loss: 0.8717, Train Acc: 0.7540, Val Loss: 0.4030, Val Acc: 0.8625, Time: 57.17s, Peak Memory: 26.98MB\n",
      "Epoch 102/200, Train Loss: 0.8725, Train Acc: 0.7529, Val Loss: 0.4049, Val Acc: 0.8617, Time: 57.14s, Peak Memory: 26.98MB\n",
      "Epoch 103/200, Train Loss: 0.8673, Train Acc: 0.7542, Val Loss: 0.4062, Val Acc: 0.8616, Time: 57.20s, Peak Memory: 26.98MB\n",
      "Epoch 104/200, Train Loss: 0.8663, Train Acc: 0.7553, Val Loss: 0.4058, Val Acc: 0.8614, Time: 57.11s, Peak Memory: 26.98MB\n",
      "Epoch 105/200, Train Loss: 0.8660, Train Acc: 0.7553, Val Loss: 0.4046, Val Acc: 0.8621, Time: 57.24s, Peak Memory: 26.98MB\n",
      "Epoch 106/200, Train Loss: 0.8681, Train Acc: 0.7537, Val Loss: 0.4044, Val Acc: 0.8625, Time: 57.32s, Peak Memory: 26.98MB\n",
      "Epoch 107/200, Train Loss: 0.8643, Train Acc: 0.7556, Val Loss: 0.4060, Val Acc: 0.8616, Time: 57.18s, Peak Memory: 26.98MB\n",
      "Epoch 108/200, Train Loss: 0.8664, Train Acc: 0.7548, Val Loss: 0.4052, Val Acc: 0.8612, Time: 57.18s, Peak Memory: 26.98MB\n",
      "Epoch 109/200, Train Loss: 0.8658, Train Acc: 0.7552, Val Loss: 0.4048, Val Acc: 0.8619, Time: 57.23s, Peak Memory: 26.98MB\n",
      "Epoch 110/200, Train Loss: 0.8672, Train Acc: 0.7540, Val Loss: 0.4056, Val Acc: 0.8614, Time: 57.10s, Peak Memory: 26.98MB\n",
      "Early stopping at epoch 111\n",
      "Training time: 6307.85 seconds\n",
      "Peak memory usage: 173.95 MB\n",
      "Results saved to ./result.json\n",
      "\n",
      "Fold 2\n",
      "Epoch 1/200, Train Loss: 2.0220, Train Acc: 0.4354, Val Loss: 0.9179, Val Acc: 0.7146, Time: 47.69s, Peak Memory: 26.98MB\n",
      "Epoch 2/200, Train Loss: 1.5088, Train Acc: 0.5682, Val Loss: 0.7459, Val Acc: 0.7597, Time: 47.88s, Peak Memory: 26.98MB\n",
      "Epoch 3/200, Train Loss: 1.3682, Train Acc: 0.6054, Val Loss: 0.6771, Val Acc: 0.7790, Time: 48.49s, Peak Memory: 26.98MB\n",
      "Epoch 4/200, Train Loss: 1.2917, Train Acc: 0.6313, Val Loss: 0.6288, Val Acc: 0.7862, Time: 48.29s, Peak Memory: 26.98MB\n",
      "Epoch 5/200, Train Loss: 1.2470, Train Acc: 0.6459, Val Loss: 0.6005, Val Acc: 0.8008, Time: 48.52s, Peak Memory: 26.98MB\n",
      "Epoch 6/200, Train Loss: 1.2171, Train Acc: 0.6537, Val Loss: 0.6039, Val Acc: 0.8018, Time: 48.15s, Peak Memory: 26.98MB\n",
      "Epoch 7/200, Train Loss: 1.1904, Train Acc: 0.6616, Val Loss: 0.5601, Val Acc: 0.8123, Time: 47.91s, Peak Memory: 26.98MB\n",
      "Epoch 8/200, Train Loss: 1.1724, Train Acc: 0.6696, Val Loss: 0.5606, Val Acc: 0.8058, Time: 47.94s, Peak Memory: 26.98MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_file_path = './result.json'\n",
    "\n",
    "k = 5\n",
    "targets = train_dataset.targets.numpy()\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "print(\"setup:\\n\"\n",
    "      f\"num_of_hidden_layer: {num_of_hidden_layer.value}, \\n\"\n",
    "      f\"hidden_sizes: {hidden_sizes}, \\n\"\n",
    "      f\"learning_rate: {lr}, \\n\"\n",
    "      f\"learning_rate_scheduler: {learning_rate_scheduler.value}, \\n\"\n",
    "      f\"optimizer: {optimizer.value}, \\n\"\n",
    "      f\"activation_function: {activation_function.value}, \\n\"\n",
    "      f\"batch_normalization: {batch_normalization.value}, \\n\"\n",
    "      f\"regularization: {regularization.value}, \\n\"\n",
    "      f\"llamada: {llambda}, \\n\"\n",
    "      f\"dropout: {dropout}, \\n\"\n",
    "      f\"augmentation: {augmentation.value}, \\n\"\n",
    "      f\"num_of_epochs: {num_epochs}, \\n\"\n",
    "      f\"batch_size: {batch_size}, \\n\"\n",
    "      f\"num_of_folds: {skf.n_splits}, \\n\"\n",
    "      f\"augmentation: {augmentation.value}, \\n\"\n",
    "      f\"train_dataset_size: {len(train_dataset)}, \\n\"\n",
    "      f\"test_dataset_size: {len(test_dataset)}\")\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(targets)), targets)):\n",
    "    print(f'\\nFold {fold+1}')\n",
    "    model_path = f'./models/best_model_fold_{fold+1}-{k}_num-of-hidden-layer_{num_of_hidden_layer.value}_hidden-sizes_{hidden_sizes}_lr_{lr}_scheduler_{learning_rate_scheduler.value}_optimizer_{optimizer.value}_activation_function_{activation_function.value}_batch_normalization_{batch_normalization.value}_regularization_{regularization.value}_llambda_{llambda}_dropout_{dropout}.pth'\n",
    "    if augmentation == Augmentation.True_:\n",
    "        model_path = model_path.replace('.pth', '_aug.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model already exists at {model_path}, skipping training.\")\n",
    "        # break\n",
    "        # continue\n",
    "    # create the data loaders\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "    train_subset.dataset = copy.deepcopy(train_dataset)\n",
    "    val_subset.dataset = copy.deepcopy(train_dataset)\n",
    "    if augmentation == Augmentation.True_:\n",
    "        train_subset.dataset.transform = aug_transform\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    model = MLP(\n",
    "        input_size=28*28, \n",
    "        hidden_sizes=hidden_sizes, \n",
    "        num_classes=47, \n",
    "        activation_fn=getattr(nn, activation_function.value),\n",
    "        batch_normalization=batch_normalization.value, \n",
    "        dropout=dropout,\n",
    "    )\n",
    "    # optimizer_copy = getattr(optim, optimizer.value)(model.parameters(), lr=lr)\n",
    "    if optimizer == Optimizer.SGD:\n",
    "        optimizer_copy = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer == Optimizer.Adam:\n",
    "        optimizer_copy = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == Optimizer.RMSprop:\n",
    "        optimizer_copy = optim.RMSprop(model.parameters(), lr=lr, alpha=0.9, momentum=0.9)\n",
    "    if learning_rate_scheduler == LearningRateScheduler.StepLR:\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer_copy, step_size=10, gamma=0.5)\n",
    "    elif learning_rate_scheduler == LearningRateScheduler.ReduceLROnPlateau:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_copy, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    best_val_loss, best_val_acc, best_train_loss, best_train_acc, train_loss_list, val_loss_list, train_acc_list, val_acc_list = trainMLP(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer_copy, \n",
    "        scheduler, \n",
    "        criterion, \n",
    "        num_epochs=num_epochs, \n",
    "        device=device, \n",
    "        model_path=model_path\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    train_duration = end_time - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    print(f\"Training time: {train_duration:.2f} seconds\")\n",
    "    print(f\"Peak memory usage: {peak / 1024 / 1024:.2f} MB\")    \n",
    "\n",
    "    new_result = {\n",
    "        \"type\": \"MLP\",\n",
    "        \"num_of_hidden_layer\": num_of_hidden_layer.value,\n",
    "        \"hidden_sizes\": hidden_sizes,\n",
    "        \"learning_rate\": lr,\n",
    "        \"learning_rate_scheduler\": learning_rate_scheduler.value,\n",
    "        \"optimizer\": optimizer.value,\n",
    "        \"activation_function\": activation_function.value,\n",
    "        \"batch_normalization\": batch_normalization.value,\n",
    "        \"regularization\": regularization.value,\n",
    "        \"llambda\": llambda,\n",
    "        \"dropout\": dropout,\n",
    "        \"augmentation\": augmentation.value,\n",
    "        \"num_of_epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_of_folds\": skf.n_splits,\n",
    "        \"train_dataset_size\": len(train_dataset),\n",
    "        \"test_dataset_size\": len(test_dataset),\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"best_train_loss\": best_train_loss,\n",
    "        \"best_train_acc\": best_train_acc,\n",
    "        \"train_loss_list\": train_loss_list,\n",
    "        \"val_loss_list\": val_loss_list,\n",
    "        \"train_acc_list\": train_acc_list,\n",
    "        \"val_acc_list\": val_acc_list,\n",
    "        \"model_path\": model_path,\n",
    "        \"fold\": fold + 1,\n",
    "        \"num_of_folds\": k,\n",
    "    }\n",
    "    if os.path.exists(result_file_path):\n",
    "        with open(result_file_path, \"r\") as result_file:\n",
    "            data = json.load(result_file)\n",
    "    else:\n",
    "        data = []\n",
    "    data.append(new_result)\n",
    "    with open(result_file_path, \"w\") as result_file:\n",
    "        json.dump(data, result_file, indent=4)\n",
    "    print(f\"Results saved to {result_file_path}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3567227",
   "metadata": {},
   "source": [
    "## eval the model and plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76da9d",
   "metadata": {},
   "source": [
    "### eval the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc141e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./models/best_model_fold_1-5_num-of-hidden-layer_3_hidden-sizes_[128, 128, 128]_lr_0.1_scheduler_StepLR_optimizer_SGD_activation_function_ELU_batch_normalization_True_regularization_L1_llambda_1e-05_dropout_[0.0, 0.0, 0.0].pth\n",
      "Fold 1 - Accuracy: 0.8548\n",
      "Loading model from ./models/best_model_fold_2-5_num-of-hidden-layer_3_hidden-sizes_[128, 128, 128]_lr_0.1_scheduler_StepLR_optimizer_SGD_activation_function_ELU_batch_normalization_True_regularization_L1_llambda_1e-05_dropout_[0.0, 0.0, 0.0].pth\n",
      "Fold 2 - Accuracy: 0.8548\n",
      "Loading model from ./models/best_model_fold_3-5_num-of-hidden-layer_3_hidden-sizes_[128, 128, 128]_lr_0.1_scheduler_StepLR_optimizer_SGD_activation_function_ELU_batch_normalization_True_regularization_L1_llambda_1e-05_dropout_[0.0, 0.0, 0.0].pth\n",
      "Fold 3 - Accuracy: 0.8505\n",
      "Loading model from ./models/best_model_fold_4-5_num-of-hidden-layer_3_hidden-sizes_[128, 128, 128]_lr_0.1_scheduler_StepLR_optimizer_SGD_activation_function_ELU_batch_normalization_True_regularization_L1_llambda_1e-05_dropout_[0.0, 0.0, 0.0].pth\n",
      "Fold 4 - Accuracy: 0.8576\n",
      "Loading model from ./models/best_model_fold_5-5_num-of-hidden-layer_3_hidden-sizes_[128, 128, 128]_lr_0.1_scheduler_StepLR_optimizer_SGD_activation_function_ELU_batch_normalization_True_regularization_L1_llambda_1e-05_dropout_[0.0, 0.0, 0.0].pth\n",
      "Fold 5 - Accuracy: 0.8553\n",
      "Average accuracy: 0.8546  0.0023\n"
     ]
    }
   ],
   "source": [
    "# define the hyperparameters\n",
    "num_of_hidden_layer_test = NumOfHiddenLayer.THREE\n",
    "# hidden_sizes_test = [32, 32, 32]\n",
    "# hidden_sizes_test = [64, 64, 64]\n",
    "hidden_sizes_test = [128, 128, 128]\n",
    "learning_rate_scheduler_test = LearningRateScheduler.StepLR\n",
    "activation_function_test = ActivationFunction.ELU\n",
    "optimizer_test = Optimizer.SGD\n",
    "batch_normalization_test = BatchNormalization.True_\n",
    "regularization_test = Regularization.L1\n",
    "llambda_test = 1e-5\n",
    "dropout_test = [0.0, 0.0, 0.0]\n",
    "batch_size_test = 128\n",
    "num_epochs_test = 200\n",
    "lr_test = 0.1\n",
    "augmentation_test = Augmentation.True_\n",
    "\n",
    "accuracy_list = []\n",
    "for i in range(1, k+1):\n",
    "    model_path_test = f'./models/best_model_fold_{i}-{k}_num-of-hidden-layer_{num_of_hidden_layer_test.value}_hidden-sizes_{hidden_sizes_test}_lr_{lr_test}_scheduler_{learning_rate_scheduler_test.value}_optimizer_{optimizer_test.value}_activation_function_{activation_function_test.value}_batch_normalization_{batch_normalization_test.value}_regularization_{regularization_test.value}_llambda_{llambda_test}_dropout_{dropout_test}.pth'\n",
    "    if augmentation_test == Augmentation.True_:\n",
    "        model_path_test = model_path_test.replace('.pth', '_aug.pth')\n",
    "    print(f\"Loading model from {model_path_test}\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "    model_test = MLP(\n",
    "        input_size=28*28, \n",
    "        hidden_sizes=hidden_sizes_test, \n",
    "        num_classes=47, \n",
    "        activation_fn=getattr(nn, activation_function_test.value),\n",
    "        batch_normalization=batch_normalization_test.value, \n",
    "        dropout=dropout_test,\n",
    "    )\n",
    "    model_test.load_state_dict(torch.load(model_path_test))\n",
    "    model_test.to(device)\n",
    "    model_test.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_test(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    # calculate the precision, recall, f1-score, accuracy, and confusion matrix\n",
    "    prcision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # cm = confusion_matrix(all_labels, all_preds)\n",
    "    # print(f\"Precision: {prcision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"F1-score: {f1:.4f}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    # # plot the confusion matrix\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_dataset.classes).plot(cmap=plt.cm.Blues)\n",
    "    # plt.title(\"Confusion Matrix\")\n",
    "    # plt.show()\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f\"Fold {i} - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Average accuracy: {np.mean(accuracy_list):.4f}  {np.std(accuracy_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff40acc",
   "metadata": {},
   "source": [
    "### get the average accuracy and loss for 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "L1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>num_of_hidden_layer</th>\n",
       "      <th>hidden_sizes</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>learning_rate_scheduler</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>batch_normalization</th>\n",
       "      <th>regularization</th>\n",
       "      <th>llambda</th>\n",
       "      <th>...</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_val_acc</th>\n",
       "      <th>best_train_loss</th>\n",
       "      <th>best_train_acc</th>\n",
       "      <th>train_loss_list</th>\n",
       "      <th>val_loss_list</th>\n",
       "      <th>train_acc_list</th>\n",
       "      <th>val_acc_list</th>\n",
       "      <th>model_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>StepLR</td>\n",
       "      <td>SGD</td>\n",
       "      <td>ELU</td>\n",
       "      <td>True</td>\n",
       "      <td>L1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425995</td>\n",
       "      <td>0.858732</td>\n",
       "      <td>0.435505</td>\n",
       "      <td>0.880740</td>\n",
       "      <td>[1.0514524084456423, 0.7243618022888265, 0.644...</td>\n",
       "      <td>[0.7067492602987492, 0.5978496709613935, 0.553...</td>\n",
       "      <td>[0.6959109042553191, 0.7877327127659575, 0.812...</td>\n",
       "      <td>[0.7665780141843972, 0.8055407801418439, 0.818...</td>\n",
       "      <td>./models/best_model_fold_1-5_num-of-hidden-lay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>StepLR</td>\n",
       "      <td>SGD</td>\n",
       "      <td>ELU</td>\n",
       "      <td>True</td>\n",
       "      <td>L1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440129</td>\n",
       "      <td>0.851729</td>\n",
       "      <td>0.432622</td>\n",
       "      <td>0.883234</td>\n",
       "      <td>[1.0543164833217649, 0.7173462152903807, 0.639...</td>\n",
       "      <td>[0.727763547204065, 0.6220474506946321, 0.5540...</td>\n",
       "      <td>[0.6983710106382979, 0.7906914893617021, 0.815...</td>\n",
       "      <td>[0.7598404255319149, 0.793395390070922, 0.8178...</td>\n",
       "      <td>./models/best_model_fold_2-5_num-of-hidden-lay...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>StepLR</td>\n",
       "      <td>SGD</td>\n",
       "      <td>ELU</td>\n",
       "      <td>True</td>\n",
       "      <td>L1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432746</td>\n",
       "      <td>0.853945</td>\n",
       "      <td>0.418136</td>\n",
       "      <td>0.884896</td>\n",
       "      <td>[1.0521992535455853, 0.7191353792417134, 0.639...</td>\n",
       "      <td>[0.7469477131011638, 0.6005892899864954, 0.536...</td>\n",
       "      <td>[0.695844414893617, 0.7893506205673759, 0.8159...</td>\n",
       "      <td>[0.7588652482269503, 0.7944148936170212, 0.823...</td>\n",
       "      <td>./models/best_model_fold_3-5_num-of-hidden-lay...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>StepLR</td>\n",
       "      <td>SGD</td>\n",
       "      <td>ELU</td>\n",
       "      <td>True</td>\n",
       "      <td>L1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416130</td>\n",
       "      <td>0.856959</td>\n",
       "      <td>0.433566</td>\n",
       "      <td>0.881738</td>\n",
       "      <td>[1.053413792366677, 0.7183074340329948, 0.6442...</td>\n",
       "      <td>[0.7002566995350181, 0.5842840658857468, 0.524...</td>\n",
       "      <td>[0.6973404255319149, 0.7907801418439716, 0.813...</td>\n",
       "      <td>[0.7752659574468085, 0.8061170212765958, 0.821...</td>\n",
       "      <td>./models/best_model_fold_4-5_num-of-hidden-lay...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>0.1</td>\n",
       "      <td>StepLR</td>\n",
       "      <td>SGD</td>\n",
       "      <td>ELU</td>\n",
       "      <td>True</td>\n",
       "      <td>L1</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417003</td>\n",
       "      <td>0.859486</td>\n",
       "      <td>0.433688</td>\n",
       "      <td>0.882824</td>\n",
       "      <td>[1.0582838099053566, 0.7216449434030141, 0.644...</td>\n",
       "      <td>[0.6996115853600469, 0.6007132342521181, 0.545...</td>\n",
       "      <td>[0.6967863475177305, 0.7901374113475177, 0.813...</td>\n",
       "      <td>[0.7782801418439717, 0.8011081560283688, 0.821...</td>\n",
       "      <td>./models/best_model_fold_5-5_num-of-hidden-lay...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  num_of_hidden_layer     hidden_sizes  learning_rate  \\\n",
       "70  MLP                    3  [128, 128, 128]            0.1   \n",
       "71  MLP                    3  [128, 128, 128]            0.1   \n",
       "72  MLP                    3  [128, 128, 128]            0.1   \n",
       "73  MLP                    3  [128, 128, 128]            0.1   \n",
       "74  MLP                    3  [128, 128, 128]            0.1   \n",
       "\n",
       "   learning_rate_scheduler optimizer activation_function  batch_normalization  \\\n",
       "70                  StepLR       SGD                 ELU                 True   \n",
       "71                  StepLR       SGD                 ELU                 True   \n",
       "72                  StepLR       SGD                 ELU                 True   \n",
       "73                  StepLR       SGD                 ELU                 True   \n",
       "74                  StepLR       SGD                 ELU                 True   \n",
       "\n",
       "   regularization  llambda  ... best_val_loss  best_val_acc  best_train_loss  \\\n",
       "70             L1  0.00001  ...      0.425995      0.858732         0.435505   \n",
       "71             L1  0.00001  ...      0.440129      0.851729         0.432622   \n",
       "72             L1  0.00001  ...      0.432746      0.853945         0.418136   \n",
       "73             L1  0.00001  ...      0.416130      0.856959         0.433566   \n",
       "74             L1  0.00001  ...      0.417003      0.859486         0.433688   \n",
       "\n",
       "    best_train_acc                                    train_loss_list  \\\n",
       "70        0.880740  [1.0514524084456423, 0.7243618022888265, 0.644...   \n",
       "71        0.883234  [1.0543164833217649, 0.7173462152903807, 0.639...   \n",
       "72        0.884896  [1.0521992535455853, 0.7191353792417134, 0.639...   \n",
       "73        0.881738  [1.053413792366677, 0.7183074340329948, 0.6442...   \n",
       "74        0.882824  [1.0582838099053566, 0.7216449434030141, 0.644...   \n",
       "\n",
       "                                        val_loss_list  \\\n",
       "70  [0.7067492602987492, 0.5978496709613935, 0.553...   \n",
       "71  [0.727763547204065, 0.6220474506946321, 0.5540...   \n",
       "72  [0.7469477131011638, 0.6005892899864954, 0.536...   \n",
       "73  [0.7002566995350181, 0.5842840658857468, 0.524...   \n",
       "74  [0.6996115853600469, 0.6007132342521181, 0.545...   \n",
       "\n",
       "                                       train_acc_list  \\\n",
       "70  [0.6959109042553191, 0.7877327127659575, 0.812...   \n",
       "71  [0.6983710106382979, 0.7906914893617021, 0.815...   \n",
       "72  [0.695844414893617, 0.7893506205673759, 0.8159...   \n",
       "73  [0.6973404255319149, 0.7907801418439716, 0.813...   \n",
       "74  [0.6967863475177305, 0.7901374113475177, 0.813...   \n",
       "\n",
       "                                         val_acc_list  \\\n",
       "70  [0.7665780141843972, 0.8055407801418439, 0.818...   \n",
       "71  [0.7598404255319149, 0.793395390070922, 0.8178...   \n",
       "72  [0.7588652482269503, 0.7944148936170212, 0.823...   \n",
       "73  [0.7752659574468085, 0.8061170212765958, 0.821...   \n",
       "74  [0.7782801418439717, 0.8011081560283688, 0.821...   \n",
       "\n",
       "                                           model_path  fold  \n",
       "70  ./models/best_model_fold_1-5_num-of-hidden-lay...     1  \n",
       "71  ./models/best_model_fold_2-5_num-of-hidden-lay...     2  \n",
       "72  ./models/best_model_fold_3-5_num-of-hidden-lay...     3  \n",
       "73  ./models/best_model_fold_4-5_num-of-hidden-lay...     4  \n",
       "74  ./models/best_model_fold_5-5_num-of-hidden-lay...     5  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_val_acc: 0.8562  0.0033\n",
      "best_train_acc: 0.8827  0.0016\n",
      "best_val_loss: 0.4264  0.0103\n",
      "best_train_loss: 0.4307  0.0071\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open('./result.json', \"r\") as result_file:\n",
    "    data = json.load(result_file)\n",
    "# print(str(hidden_sizes_test))\n",
    "# print(type(df['hidden_sizes'][0]))\n",
    "df = pd.DataFrame(data)\n",
    "print(len(df))\n",
    "print(regularization_test.value)\n",
    "df['regularization'] = df['regularization'].apply(lambda x: x if x is not None else 'None')\n",
    "df = df[(df['num_of_hidden_layer'] == num_of_hidden_layer_test.value) &\n",
    "        (df['hidden_sizes'].apply(lambda x: x == hidden_sizes_test)) &\n",
    "        (df['learning_rate'] == lr_test) &\n",
    "        (df['learning_rate_scheduler'] == learning_rate_scheduler_test.value) &\n",
    "        (df['optimizer'] == optimizer_test.value) &\n",
    "        (df['activation_function'] == activation_function_test.value) &\n",
    "        (df['batch_normalization'] == batch_normalization_test.value) &\n",
    "        (df['regularization'] == str(regularization_test.value)) &\n",
    "        (df['llambda'] == llambda_test) &\n",
    "        (df['dropout'].apply(lambda x: x == dropout_test)) &\n",
    "        (df['num_of_epochs'] == num_epochs_test) &\n",
    "        (df['batch_size'] == batch_size_test)]\n",
    "if augmentation_test == Augmentation.True_:\n",
    "    df = df[df['augmentation'] == True]\n",
    "display(df)\n",
    "print(f'best_val_acc: {df[\"best_val_acc\"].mean():.4f}  {df[\"best_val_acc\"].std():.4f}')\n",
    "print(f'best_train_acc: {df[\"best_train_acc\"].mean():.4f}  {df[\"best_train_acc\"].std():.4f}')\n",
    "print(f'best_val_loss: {df[\"best_val_loss\"].mean():.4f}  {df[\"best_val_loss\"].std():.4f}')\n",
    "print(f'best_train_loss: {df[\"best_train_loss\"].mean():.4f}  {df[\"best_train_loss\"].std():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
